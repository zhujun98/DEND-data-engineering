{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3582a1fa",
   "metadata": {},
   "source": [
    "**The goal of this Jupyter Notebook is to:**\n",
    "\n",
    "- Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "- Formalize the steps to clean the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdde98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from datetime import datetime\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType, DateType, DoubleType, IntegerType, LongType, StringType, StructField, StructType, TimestampType\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# !pip3 install matplotlib pandas\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ab86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Sparkify ETL\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441a5b4",
   "metadata": {},
   "source": [
    "## Capital bikeshare trip data\n",
    "\n",
    "![Capital bikeshare system map](./capital_bikeshare_system_map.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918352c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRIP_DATA_PATH = \"./datasets/capitalbikeshare_tripdata/2020*.csv\"\n",
    "\n",
    "trip_data_schema = StructType([\n",
    "    StructField('ride_id', StringType()),\n",
    "    StructField('rideable_type', StringType()),\n",
    "    StructField('started_at', TimestampType()),\n",
    "    StructField('ended_at', TimestampType()),\n",
    "    StructField('start_station_name', StringType()),\n",
    "    StructField('start_station_id', LongType()),\n",
    "    StructField('end_station_name', StringType()),\n",
    "    StructField('end_station_id', LongType()),\n",
    "    StructField('start_lat', DoubleType()),\n",
    "    StructField('start_lng', DoubleType()),\n",
    "    StructField('end_lat', DoubleType()),\n",
    "    StructField('end_lng', DoubleType()),\n",
    "    StructField('member_casual', StringType())\n",
    "])\n",
    "\n",
    "trip_data = spark.read.csv(TRIP_DATA_PATH, header=True, schema=trip_data_schema)\n",
    "trip_data.show(5)\n",
    "\n",
    "trip_data.printSchema()\n",
    "\n",
    "print(\"Total number records: \", trip_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5bd0aa",
   "metadata": {},
   "source": [
    "## COVID data by states\n",
    "\n",
    "Data description can be found at https://covidtracking.com/data/api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_DATA_PATH = \"./datasets/covid_data/daily.json\"\n",
    "\n",
    "# Select only interested columns\n",
    "covid_data = spark.read.json(COVID_DATA_PATH).select(\n",
    "    \"dataQualityGrade\", \"date\", \"state\", \"death\", \"deathIncrease\", \"hospitalizedCurrently\", \"hospitalizedDischarged\", \"hospitalizedIncrease\", \n",
    "    \"positive\", \"positiveIncrease\", \"recovered\"\n",
    ")\n",
    "# Select only data from Washington DC\n",
    "covid_data = covid_data.filter(F.col(\"state\") == \"DC\").drop(\"state\")\n",
    "\n",
    "covid_data.show(5)\n",
    "\n",
    "covid_data.printSchema()\n",
    "\n",
    "print(\"Total number records: \", covid_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a375b",
   "metadata": {},
   "source": [
    "**Drop columns which has a single value (e.g. null), which typically means data is not available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data.select(\"dataQualityGrade\", \"hospitalizedDischarged\", \"hospitalizedIncrease\").distinct().show()\n",
    "covid_data = covid_data.drop(\"dataQualityGrade\", \"hospitalizedDischarged\", \"hospitalizedIncrease\")\n",
    "\n",
    "covid_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ef516",
   "metadata": {},
   "source": [
    "**Convert type of column \"date\" from `long` to `date`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "func =  F.udf(lambda x: datetime.strptime(str(x), '%Y%m%d'), DateType())\n",
    "\n",
    "covid_data = covid_data.withColumn(\"date\", func(F.col(\"date\")))\n",
    "covid_data.orderBy(\"date\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae33184",
   "metadata": {},
   "source": [
    "**Fill null with 0. Actually, the null values were discovered by the following visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data = covid_data.na.fill(0).orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f7e78",
   "metadata": {},
   "source": [
    "**Sanity check by plotting the temperature data and wind speed data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff945ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = covid_data.toPandas()\n",
    "\n",
    "_, axes = plt.subplots(3, 2, figsize=(16, 9))\n",
    "\n",
    "for col, ax in zip(list(covid_df.columns[1:]), axes.flatten()):\n",
    "    covid_df.plot(\"date\", col, ax=ax)\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249aed7c",
   "metadata": {},
   "source": [
    "**Note: the dip in the \"hospitalizedCurrently\" plot and the jump in the \"recovered\" plot are both suspicious!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e3caea",
   "metadata": {},
   "source": [
    "## Weather data\n",
    "\n",
    "- AWND: Average daily wind speed (miles per hour)\n",
    "- TAVG: Average temperature (Fahrenheit)\n",
    "- TMAX: Maximum temperature (Fahrenheit)\n",
    "- TMIN: Minimum temperature (Fahrenheit)\n",
    "- TOBS: Temperature at the time of observation (Fahrenheit)\n",
    "- WDF2: Direction of fastest 2-minute wind (degrees)\n",
    "- WDF5: Direction of fastest 5-second wind (degrees)\n",
    "- WSF2: Fastest 2-minute wind speed (miles per hour)\n",
    "- WSF5: Fastest 5-second wind speed (miles per hour)\n",
    "- WDMV: 24-hour wind movement (miles)\n",
    "- WT01: Fog, ice fog, or freezing fog (may include heavy fog)\n",
    "- WT02: Heavy fog or heaving freezing fog (not always distinguished from fog)\n",
    "- WT03: Thunder\n",
    "- WT04: Ice pellets, sleet, snow pellets, or small hail\n",
    "- WT05: Hail (may include small hail)\n",
    "- WT06: Glaze or rime\n",
    "- WT08: Smoke or haze\n",
    "- WT11: High or damaging winds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_DATA_PATH = \"./datasets/weather_data/*_daily.csv\"\n",
    "\n",
    "weather_data_schema = StructType([\n",
    "    StructField('STATION', StringType()),\n",
    "    StructField('NAME', StringType()),\n",
    "    StructField('DATE', DateType()),\n",
    "    StructField('AWND', DoubleType()),\n",
    "    StructField('TAVG', DoubleType()),\n",
    "    StructField('TMAX', DoubleType()),\n",
    "    StructField('TMIN', DoubleType()),\n",
    "    StructField('TOBS', DoubleType()),\n",
    "    StructField('WDF2', DoubleType()),\n",
    "    StructField('WDF5', DoubleType()), \n",
    "    StructField('WDMV', DoubleType()), \n",
    "    StructField('WSF2', DoubleType()),\n",
    "    StructField('WSF5', DoubleType()),\n",
    "    StructField('WT01', StringType()),\n",
    "    StructField('WT02', StringType()), \n",
    "    StructField('WT03', StringType()), \n",
    "    StructField('WT04', StringType()), \n",
    "    StructField('WT05', StringType()), \n",
    "    StructField('WT06', StringType()), \n",
    "    StructField('WT08', StringType()), \n",
    "    StructField('WT11', StringType())\n",
    "])\n",
    "\n",
    "weather_data = spark.read.csv(WEATHER_DATA_PATH, header=True, schema=weather_data_schema).drop(\n",
    "    \"NAME\", \"TOBS\", \"WDF2\", \"WDF5\", \"WDMV\", \"WSF2\", \"WSF5\")\n",
    "\n",
    "weather_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec0b19",
   "metadata": {},
   "source": [
    "**Remove rows if any of the columns \"AWND\", \"TAVG\", \"TMAX\" and \"TMIN\" contain null. Afterwards, replace null in WT?? with 0 and cast the data type to boolean.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of stations before filtering: \", weather_data.select('STATION').distinct().count())\n",
    "\n",
    "weather_data = weather_data.filter(F.col(\"AWND\").isNotNull()).filter(F.col(\"TAVG\").isNotNull()).filter(F.col(\"TMAX\").isNotNull()).filter(F.col(\"TMIN\").isNotNull())\n",
    "\n",
    "print(\"Number of stations after filtering: \", weather_data.select('STATION').distinct().count())\n",
    "\n",
    "weather_data = weather_data.na.fill('0')\n",
    "for i in ['01', \"02\", \"03\", \"04\", \"05\", \"06\", \"08\", \"11\"]:\n",
    "    col_name = f\"WT{i}\"\n",
    "    orig_col_name = f\"{col_name}_orig\"\n",
    "    weather_data = weather_data.withColumnRenamed(col_name, orig_col_name)\n",
    "    weather_data = weather_data.withColumn(col_name, F.col(orig_col_name).cast(BooleanType())).drop(orig_col_name)\n",
    "\n",
    "weather_data.show(5)\n",
    "\n",
    "weather_data.printSchema()\n",
    "\n",
    "print(\"Total number records: \", weather_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a018fc",
   "metadata": {},
   "source": [
    "**To make my life easy, I select one of the three stations. Of course, one could use the aggregated values of the three stations, or map weather station to bike station.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather_data.filter(F.col(\"STATION\") == \"USW00093721\").drop(\"STATION\")\n",
    "\n",
    "print(\"Total number records: \", weather_data.count())\n",
    "\n",
    "assert(weather_data.select(\"DATE\").distinct().count() == weather_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2868613",
   "metadata": {},
   "source": [
    "**Sanity check by plotting the temperature data and wind speed data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_data.toPandas()\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "weather_df.plot(\"DATE\", [\"TAVG\", \"TMIN\", \"TMAX\"], ax=axes[0])\n",
    "weather_df.plot(\"DATE\", \"AWND\", ax=axes[1])\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='x', labelrotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8063bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
