{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb8ab3c",
   "metadata": {},
   "source": [
    "**The goal of this Jupyter Notebook is to:**\n",
    "\n",
    "- Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "- Formalize the steps to clean the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType, DateType, DoubleType, IntegerType, LongType, StringType, StructField, StructType, TimestampType\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d11d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Sparkify ETL\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550df8d",
   "metadata": {},
   "source": [
    "## Capital bikeshare trip data\n",
    "\n",
    "![Capital bikeshare system map](./capital_bikeshare_system_map.png)\n",
    "\n",
    "Data description can be found at https://www.capitalbikeshare.com/system-data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c9a3ea",
   "metadata": {},
   "source": [
    "**After initial exploration, it is found that the data schema has been changed since 05.2020. The new data schema is different from the schema listed on the official website. In the following, the data with different schemas with be merged.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d37999",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIP_DATA_FOLDER = pathlib.Path(\"./datasets/capitalbikeshare_tripdata\")\n",
    "TRIP_DATA_PATHS_OLD = []\n",
    "TRIP_DATA_PATHS_NEW = []\n",
    "for filename in TRIP_DATA_FOLDER.glob(\"*.csv\"):\n",
    "    if int(filename.stem[:6]) <= 202003:\n",
    "        TRIP_DATA_PATHS_OLD.append(str(filename))\n",
    "    else:\n",
    "        TRIP_DATA_PATHS_NEW.append(str(filename))\n",
    "        \n",
    "len(TRIP_DATA_PATHS_OLD), len(TRIP_DATA_PATHS_NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c421617",
   "metadata": {},
   "source": [
    "**Explore data in the new format.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62372c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trip_data_new_schema = StructType([\n",
    "    StructField('ride_id', StringType()),\n",
    "    StructField('rideable_type', StringType()),\n",
    "    StructField('started_at', TimestampType()),\n",
    "    StructField('ended_at', TimestampType()),\n",
    "    StructField('start_station_name', StringType()),\n",
    "    StructField('start_station_id', LongType()),\n",
    "    StructField('end_station_name', StringType()),\n",
    "    StructField('end_station_id', LongType()),\n",
    "    StructField('start_lat', DoubleType()),\n",
    "    StructField('start_lng', DoubleType()),\n",
    "    StructField('end_lat', DoubleType()),\n",
    "    StructField('end_lng', DoubleType()),\n",
    "    StructField(\"member_casual\", StringType())\n",
    "])\n",
    "\n",
    "trip_data_new = spark.read.csv(TRIP_DATA_PATHS_NEW, header=True, schema=trip_data_new_schema)\n",
    "trip_data_new.show(5)\n",
    "\n",
    "trip_data_new.printSchema()\n",
    "\n",
    "print(\"Total number new records: \", trip_data_new.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab38237",
   "metadata": {},
   "source": [
    "**Explore data in the old format.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_old_schema = StructType([\n",
    "    StructField('Duration', DoubleType()),\n",
    "    StructField('Start date', TimestampType()),\n",
    "    StructField('End date', TimestampType()),\n",
    "    StructField('Start station number', LongType()),\n",
    "    StructField('Start station', StringType()),\n",
    "    StructField('End station number', LongType()),\n",
    "    StructField('End station', StringType()),\n",
    "    StructField('Bike number', StringType()),\n",
    "    StructField(\"Member type\", StringType())\n",
    "])\n",
    "\n",
    "trip_data_old = spark.read.csv(TRIP_DATA_PATHS_OLD, header=True, schema=trip_data_old_schema)\n",
    "trip_data_old.show(5)\n",
    "\n",
    "trip_data_old.printSchema()\n",
    "\n",
    "print(\"Total number of old records: \", trip_data_old.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b089aa3",
   "metadata": {},
   "source": [
    "**Merge old and new data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c30a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data = trip_data_old.select(F.col(\"Start station number\").alias(\"station_id\"),\n",
    "                                    F.col(\"Start station\").alias(\"station_name\")).distinct().union(\n",
    "    trip_data_old.select(\"End station number\", \"End station\").distinct()).union(\n",
    "    trip_data_new.select(\"start_station_id\", \"start_station_name\").distinct()).union(\n",
    "    trip_data_new.select(\"end_station_id\", \"end_station_name\").distinct()).distinct().sort(\"station_id\", ascending=True).dropna(\n",
    "    how=\"any\", subset=[\"station_id\"]).filter(F.col(\"station_id\") != 0).dropDuplicates(subset=[\"station_id\"])\n",
    "\n",
    "print(\"Total number of stations: \", station_data.count())\n",
    "\n",
    "station_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data = trip_data_old.select(F.lit(None).alias(\"ride_id\").cast(StringType()),\n",
    "                                 F.lit(None).alias(\"rideable_type\").cast(StringType()),\n",
    "                                 F.col(\"Start date\").alias(\"started_at\"),\n",
    "                                 F.col(\"End date\").alias(\"ended_at\"),\n",
    "                                 F.col(\"Start station number\").alias(\"start_station_id\"),\n",
    "                                 F.col(\"End station number\").alias(\"end_station_id\"),\n",
    "                                 F.col(\"Member type\").alias(\"member_casual\")).union(\n",
    "    trip_data_new.select(\"ride_id\", \"rideable_type\", \"started_at\", \"ended_at\", \"start_station_id\", \"end_station_id\", \"member_casual\"))\n",
    "\n",
    "\n",
    "# Clean up.\n",
    "trip_data = trip_data.dropna(how=\"any\", subset=[\"start_station_id\", \"end_station_id\"]).filter(\n",
    "    (F.col(\"start_station_id\") != 0) & (F.col(\"end_station_id\") != 0))\n",
    "\n",
    "# Add primary key \"tid\" and foreign key \"start_date\".\n",
    "# FIXME: monotonically_increase_id() does not return a sequence!\n",
    "trip_data = trip_data.withColumn(\"tid\", F.monotonically_increasing_id()).withColumn(\"start_date\", F.to_date(F.col(\"started_at\")))\n",
    "\n",
    "print(\"Total number of records: \", trip_data.count())\n",
    "\n",
    "trip_data.show(5)\n",
    "\n",
    "trip_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2269cb2",
   "metadata": {},
   "source": [
    "## COVID data by states\n",
    "\n",
    "Data description can be found at https://covidtracking.com/data/api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_DATA_PATH = \"./datasets/covid_data/daily.json\"\n",
    "\n",
    "# Select only interested columns\n",
    "covid_data = spark.read.json(COVID_DATA_PATH).select(\n",
    "    \"dataQualityGrade\", \"date\", \"state\", \"death\", \"deathIncrease\", \"hospitalizedCurrently\", \"hospitalizedDischarged\", \"hospitalizedIncrease\", \n",
    "    \"positive\", \"positiveIncrease\", \"recovered\"\n",
    ")\n",
    "# Select only data from Washington DC\n",
    "covid_data = covid_data.filter(F.col(\"state\") == \"DC\").drop(\"state\")\n",
    "\n",
    "covid_data.show(5)\n",
    "\n",
    "covid_data.printSchema()\n",
    "\n",
    "print(\"Total number records: \", covid_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddb834",
   "metadata": {},
   "source": [
    "**Drop columns which has a single value (e.g. null), which typically means data is not available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data.select(\"dataQualityGrade\", \"hospitalizedDischarged\", \"hospitalizedIncrease\").distinct().show()\n",
    "covid_data = covid_data.drop(\"dataQualityGrade\", \"hospitalizedDischarged\", \"hospitalizedIncrease\")\n",
    "\n",
    "covid_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc1ba6",
   "metadata": {},
   "source": [
    "**Convert type of column \"date\" from `long` to `date`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "func =  F.udf(lambda x: datetime.strptime(str(x), '%Y%m%d'), DateType())\n",
    "\n",
    "covid_data = covid_data.withColumn(\"date\", func(F.col(\"date\")))\n",
    "covid_data.orderBy(\"date\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b78e5",
   "metadata": {},
   "source": [
    "**Fill null with 0. Actually, the null values were discovered by the following visualization. It is reasonable to do it since null values only appears at the beginning of the outbreak.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec23640",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data = covid_data.fillna(0).orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af284475",
   "metadata": {},
   "source": [
    "**Drop possible duplicated rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748945c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before dropDuplicates: \", covid_data.count())\n",
    "covid_data = covid_data.dropDuplicates([\"date\"])\n",
    "print(\"After dropDuplicates: \", covid_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d59b8fe",
   "metadata": {},
   "source": [
    "**Sanity check by plotting the temperature data and wind speed data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72733db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = covid_data.toPandas()\n",
    "\n",
    "_, axes = plt.subplots(3, 2, figsize=(16, 9))\n",
    "\n",
    "for col, ax in zip(list(covid_df.columns[1:]), axes.flatten()):\n",
    "    covid_df.plot(\"date\", col, ax=ax)\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de45155",
   "metadata": {},
   "source": [
    "**Note: the dip in the \"hospitalizedCurrently\" plot and the jump in the \"recovered\" plot are both suspicious!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c866ac7",
   "metadata": {},
   "source": [
    "## Weather data\n",
    "\n",
    "- AWND: Average daily wind speed (miles per hour)\n",
    "- TAVG: Average temperature (Fahrenheit)\n",
    "- TMAX: Maximum temperature (Fahrenheit)\n",
    "- TMIN: Minimum temperature (Fahrenheit)\n",
    "- TOBS: Temperature at the time of observation (Fahrenheit)\n",
    "- WDF2: Direction of fastest 2-minute wind (degrees)\n",
    "- WDF5: Direction of fastest 5-second wind (degrees)\n",
    "- WSF2: Fastest 2-minute wind speed (miles per hour)\n",
    "- WSF5: Fastest 5-second wind speed (miles per hour)\n",
    "- WDMV: 24-hour wind movement (miles)\n",
    "- WT01: Fog, ice fog, or freezing fog (may include heavy fog)\n",
    "- WT02: Heavy fog or heaving freezing fog (not always distinguished from fog)\n",
    "- WT03: Thunder\n",
    "- WT04: Ice pellets, sleet, snow pellets, or small hail\n",
    "- WT05: Hail (may include small hail)\n",
    "- WT06: Glaze or rime\n",
    "- WT08: Smoke or haze\n",
    "- WT11: High or damaging winds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c89e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_DATA_PATH = \"./datasets/weather_data/*_daily.csv\"\n",
    "\n",
    "weather_data_schema = StructType([\n",
    "    StructField('STATION', StringType()),\n",
    "    StructField('NAME', StringType()),\n",
    "    StructField('DATE', DateType()),\n",
    "    StructField('AWND', DoubleType()),\n",
    "    StructField('TAVG', DoubleType()),\n",
    "    StructField('TMAX', DoubleType()),\n",
    "    StructField('TMIN', DoubleType()),\n",
    "    StructField('TOBS', DoubleType()),\n",
    "    StructField('WDF2', DoubleType()),\n",
    "    StructField('WDF5', DoubleType()), \n",
    "    StructField('WDMV', DoubleType()), \n",
    "    StructField('WSF2', DoubleType()),\n",
    "    StructField('WSF5', DoubleType()),\n",
    "    StructField('WT01', StringType()),\n",
    "    StructField('WT02', StringType()), \n",
    "    StructField('WT03', StringType()), \n",
    "    StructField('WT04', StringType()), \n",
    "    StructField('WT05', StringType()), \n",
    "    StructField('WT06', StringType()), \n",
    "    StructField('WT08', StringType()), \n",
    "    StructField('WT11', StringType())\n",
    "])\n",
    "\n",
    "weather_data = spark.read.csv(WEATHER_DATA_PATH, header=True, schema=weather_data_schema).drop(\n",
    "    \"NAME\", \"TOBS\", \"WDF2\", \"WDF5\", \"WDMV\", \"WSF2\", \"WSF5\")\n",
    "\n",
    "weather_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787347cd",
   "metadata": {},
   "source": [
    "**Remove rows if any of the columns \"AWND\", \"TAVG\", \"TMAX\" and \"TMIN\" contain null. Afterwards, replace null in WT?? with 0 and cast the data type to boolean.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of stations before filtering: \", weather_data.select('STATION').distinct().count())\n",
    "\n",
    "weather_data = weather_data.filter(F.col(\"AWND\").isNotNull()).filter(F.col(\"TAVG\").isNotNull()).filter(F.col(\"TMAX\").isNotNull()).filter(F.col(\"TMIN\").isNotNull())\n",
    "\n",
    "print(\"Number of stations after filtering: \", weather_data.select('STATION').distinct().count())\n",
    "\n",
    "for i in ['01', \"02\", \"03\", \"04\", \"05\", \"06\", \"08\", \"11\"]:\n",
    "    col_name = f\"WT{i}\"\n",
    "    orig_col_name = f\"{col_name}_orig\"\n",
    "    weather_data = weather_data.fillna('0', subset=[col_name]).withColumnRenamed(col_name, orig_col_name)\n",
    "    weather_data = weather_data.withColumn(col_name, F.col(orig_col_name).cast(BooleanType())).drop(orig_col_name)\n",
    "\n",
    "weather_data.show(5)\n",
    "\n",
    "weather_data.printSchema()\n",
    "\n",
    "print(\"Total number records: \", weather_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f797d1",
   "metadata": {},
   "source": [
    "**To make my life easy, I select one of the three stations. Of course, one could use the aggregated values of the three stations, or map weather station to bike station.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather_data.filter(F.col(\"STATION\") == \"USW00093721\").drop(\"STATION\")\n",
    "\n",
    "print(\"Total number records: \", weather_data.count())\n",
    "\n",
    "# There is no duplicated row.\n",
    "assert(weather_data.select(\"DATE\").distinct().count() == weather_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81152043",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c6e4a3",
   "metadata": {},
   "source": [
    "**Sanity check by plotting the temperature data and wind speed data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbebfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_data.toPandas()\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "weather_df.plot(\"DATE\", [\"TAVG\", \"TMIN\", \"TMAX\"], ax=axes[0])\n",
    "weather_df.plot(\"DATE\", \"AWND\", ax=axes[1])\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='x', labelrotation=45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
