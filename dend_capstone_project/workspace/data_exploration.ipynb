{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5a36c9",
   "metadata": {},
   "source": [
    "**The goal of this Jupyter Notebook is to:**\n",
    "\n",
    "- Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "- Formalize the steps to clean the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType, DateType, DoubleType, IntegerType, LongType, StringType, StructField, StructType, TimestampType\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "# !pip3 install matplotlib pandas\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Sparkify ETL\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ed07e",
   "metadata": {},
   "source": [
    "## Capital bikeshare trip data\n",
    "\n",
    "![Capital bikeshare system map](./capital_bikeshare_system_map.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3963b2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRIP_DATA_PATH = \"./datasets/capitalbikeshare_tripdata/2020*.csv\"\n",
    "\n",
    "trip_data_schema = StructType([\n",
    "    StructField('ride_id', StringType()),\n",
    "    StructField('rideable_type', StringType()),\n",
    "    StructField('started_at', TimestampType()),\n",
    "    StructField('ended_at', TimestampType()),\n",
    "    StructField('start_station_name', StringType()),\n",
    "    StructField('start_station_id', LongType()),\n",
    "    StructField('end_station_name', StringType()),\n",
    "    StructField('end_station_id', LongType()),\n",
    "    StructField('start_lat', DoubleType()),\n",
    "    StructField('start_lng', DoubleType()),\n",
    "    StructField('end_lat', DoubleType()),\n",
    "    StructField('end_lng', DoubleType()),\n",
    "    StructField('member_casual', StringType())\n",
    "])\n",
    "\n",
    "trip_data = spark.read.csv(TRIP_DATA_PATH, header=True, schema=trip_data_schema)\n",
    "trip_data.show(5)\n",
    "\n",
    "trip_data.printSchema()\n",
    "\n",
    "print(\"Total number records: \", trip_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c6041",
   "metadata": {},
   "source": [
    "## COVID data by states\n",
    "\n",
    "Data description can be found at https://covidtracking.com/data/api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_DATA_PATH = \"./datasets/covid_data/daily.json\"\n",
    "\n",
    "# Select only interested columns\n",
    "covid_data = spark.read.json(COVID_DATA_PATH).select(\n",
    "    \"dataQualityGrade\", \"date\", \"state\", \"death\", \"deathIncrease\", \"hospitalizedCurrently\", \"hospitalizedDischarged\", \"hospitalizedIncrease\", \n",
    "    \"positive\", \"positiveIncrease\", \"recovered\"\n",
    ")\n",
    "# Select only data from Washington DC\n",
    "covid_data = covid_data.filter(col(\"state\") == \"DC\").drop(\"state\")\n",
    "\n",
    "covid_data.show(5)\n",
    "\n",
    "covid_data.printSchema()\n",
    "\n",
    "print(\"Total number records: \", covid_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1444c6",
   "metadata": {},
   "source": [
    "Drop columns which has a single value (e.g. null), which typically means data is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data.select(\"dataQualityGrade\", \"hospitalizedDischarged\", \"hospitalizedIncrease\").distinct().show()\n",
    "covid_data = covid_data.drop(\"dataQualityGrade\", \"hospitalizedDischarged\", \"hospitalizedIncrease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0810c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = covid_data.toPandas()\n",
    "covid_df[\"deathIncrease\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d75bd",
   "metadata": {},
   "source": [
    "## Weather data\n",
    "\n",
    "- AWND: Average daily wind speed (miles per hour)\n",
    "- TAVG: Average temperature (Fahrenheit)\n",
    "- TMAX: Maximum temperature (Fahrenheit)\n",
    "- TMIN: Minimum temperature (Fahrenheit)\n",
    "- TOBS: Temperature at the time of observation (Fahrenheit)\n",
    "- WDF2: Direction of fastest 2-minute wind (degrees)\n",
    "- WDF5: Direction of fastest 5-second wind (degrees)\n",
    "- WSF2: Fastest 2-minute wind speed (miles per hour)\n",
    "- WSF5: Fastest 5-second wind speed (miles per hour)\n",
    "- WDMV: 24-hour wind movement (miles)\n",
    "- WT01: Fog, ice fog, or freezing fog (may include heavy fog)\n",
    "- WT02: Heavy fog or heaving freezing fog (not always distinguished from fog)\n",
    "- WT03: Thunder\n",
    "- WT04: Ice pellets, sleet, snow pellets, or small hail\n",
    "- WT05: Hail (may include small hail)\n",
    "- WT06: Glaze or rime\n",
    "- WT08: Smoke or haze\n",
    "- WT11: High or damaging winds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d47aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_DATA_PATH = \"./datasets/weather_data/*_daily.csv\"\n",
    "\n",
    "weather_data_schema = StructType([\n",
    "    StructField('STATION', StringType()),\n",
    "    StructField('NAME', StringType()),\n",
    "    StructField('DATE', DateType()),\n",
    "    StructField('AWND', DoubleType()),\n",
    "    StructField('TAVG', DoubleType()),\n",
    "    StructField('TMAX', DoubleType()),\n",
    "    StructField('TMIN', DoubleType()),\n",
    "    StructField('TOBS', DoubleType()),\n",
    "    StructField('WDF2', DoubleType()),\n",
    "    StructField('WDF5', DoubleType()), \n",
    "    StructField('WDMV', DoubleType()), \n",
    "    StructField('WSF2', DoubleType()),\n",
    "    StructField('WSF5', DoubleType()),\n",
    "    StructField('WT01', StringType()),\n",
    "    StructField('WT02', StringType()), \n",
    "    StructField('WT03', StringType()), \n",
    "    StructField('WT04', StringType()), \n",
    "    StructField('WT05', StringType()), \n",
    "    StructField('WT06', StringType()), \n",
    "    StructField('WT08', StringType()), \n",
    "    StructField('WT11', StringType())\n",
    "])\n",
    "\n",
    "weather_data = spark.read.csv(WEATHER_DATA_PATH, header=True, schema=weather_data_schema).drop(\n",
    "    \"NAME\", \"TOBS\", \"WDF2\", \"WDF5\", \"WDMV\", \"WSF2\", \"WSF5\")\n",
    "\n",
    "print(\"Number of stations before filtering: \", weather_data.select('STATION').distinct().count())\n",
    "\n",
    "# Remove incomplete data.\n",
    "weather_data = weather_data.filter(col(\"AWND\").isNotNull()).filter(col(\"TAVG\").isNotNull()).filter(col(\"TMAX\").isNotNull()).filter(col(\"TMIN\").isNotNull())\n",
    "\n",
    "print(\"Number of stations after filtering: \", weather_data.select('STATION').distinct().count())\n",
    "\n",
    "# Fill the WT?? null value with 0.\n",
    "weather_data = weather_data.na.fill('0')\n",
    "\n",
    "weather_data.show(5)\n",
    "\n",
    "weather_data.printSchema()\n",
    "\n",
    "print(\"Total number records: \", weather_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b3992",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['01', \"02\", \"03\", \"04\", \"05\", \"06\", \"08\", \"11\"]:\n",
    "    col_name = f\"WT{i}\"\n",
    "    orig_col_name = f\"{col_name}_orig\"\n",
    "    weather_data = weather_data.withColumnRenamed(col_name, orig_col_name)\n",
    "    weather_data = weather_data.withColumn(col_name, col(orig_col_name).cast(BooleanType())).drop(orig_col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e9d33",
   "metadata": {},
   "source": [
    "To make my life simple, I select one of the three stations. Of course, one could use the aggregated values of the three stations, or map weather station to bike station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather_data.filter(col(\"STATION\") == \"USW00093721\").drop(\"STATION\")\n",
    "\n",
    "print(\"Total number records: \", weather_data.count())\n",
    "\n",
    "assert(weather_data.select(\"DATE\").distinct().count() == weather_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd09274",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_data.toPandas()\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "weather_df.plot(\"DATE\", [\"TAVG\", \"TMIN\", \"TMAX\"], ax=axes[0])\n",
    "weather_df.plot(\"DATE\", \"AWND\", ax=axes[1])\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='x', labelrotation = 45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
